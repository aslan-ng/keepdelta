@misc{DeltaEncodingInHTTP,
    series =     {Request for Comments},
    number =     3229,
    howpublished = {RFC 3229},
    publisher =  {RFC Editor},
    doi =        {10.17487/RFC3229},
    url =        {https://www.rfc-editor.org/info/rfc3229},
    author =     {Arthur van Hoff and Fred Douglis and Balachander Krishnamurthy and Yaron Y. Goland and Daniel M. Hellerstein and Anja Feldmann and Jeffrey Mogul},
    title =      {{Delta encoding in HTTP}},
    pagetotal =  49,
    year =       2002,
    month =      jan,
    abstract =   {This document describes how delta encoding can be supported as a compatible extension to HTTP/1.1. {[}STANDARDS-TRACK{]}},
}

@article{NaiveDifferencesOfExecutableCode,
    title =      {Naive differences of executable code},
    author =     {Percival, Colin},
    journal =    {Draft Paper, http://www.daemonology.net/bsdiff},
    year =       {2003},
    publisher =  {Citeseer}
}

@software{deepdiff,
    author =     {Dehpour, Sep},
    title =      {{DeepDiff}},
    url =        {https://github.com/seperman/deepdiff},
    version =    {8.1.1}
}

@software{xdelta3,
    author =     {Colvin, Samuel},
    title =      {{xdelta3}},
    url =        {https://github.com/samuelcolvin/xdelta3-python},
    version =    {0.0.5}
}

@software{xdelta,
    author =     {MacDonald, Joshua},
    title =      {{xdelta}},
    url =        {https://github.com/jmacd/xdelta},
    version =    {3.1.0}
}

@software{detools,
    author =     {Moqvist, Erik},
    title =      {{detools}},
    url =        {https://github.com/eerimoq/detools},
    version =    {0.53.0}
}

@Article{numpy,
    title =      {Array programming with {NumPy}},
    author =     {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
    year =       {2020},
    month =      sep,
    journal =    {Nature},
    volume =     {585},
    number =     {7825},
    pages =      {357--362},
    doi =        {10.1038/s41586-020-2649-2},
    publisher =  {Springer Science and Business Media {LLC}},
    url =        {https://doi.org/10.1038/s41586-020-2649-2}
}


@article{SimulationStudies,
    author =     {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
    title =      {Using simulation studies to evaluate statistical methods},
    journal =    {Statistics in Medicine},
    volume =     {38},
    number =     {11},
    pages =      {2074-2102},
    keywords =   {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
    doi =        {https://doi.org/10.1002/sim.8086},
    url =        {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
    eprint =     {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
    abstract =   {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
    year =       {2019}
}

@InProceedings{SimulationExperiments,
    author =     "Kleijnen, Jack P. C.",
    editor =     "Pilz, J{\"u}rgen
                  and Rasch, Dieter
                  and Melas, Viatcheslav B.
                  and Moder, Karl",
    title =      "Design and Analysis of Simulation Experiments",
    booktitle =  "Statistics and Simulation",
    year =       "2018",
    publisher =  "Springer International Publishing",
    address =    "Cham",
    pages =      "3--22",
    abstract =   "This contribution summarizes the design and analysis of experiments with computerized simulation models. It focuses on two metamodel (surrogate, emulator) types, namely first-order or second-order polynomial regression, and Kriging (or Gaussian process). The metamodel type determines the design of the simulation experiment, which determines the input combinations of the simulation model. Before applying these metamodels, the analysts should screen the many inputs of a realistic simulation model; this contribution focuses on sequential bifurcation. Optimization of the simulated system may use either a sequence of first-order and second-order polynomials---so-called response surface methodology (RSM)---or Kriging models fitted through sequential designs---including efficient global optimization (EGO). Robust optimization accounts for uncertainty in some simulation inputs.",
    isbn =       "978-3-319-76035-3"
}

@article{ComplexSystemsSimulation,
    title =      {A methodology for developing simulation models of complex systems},
    journal =    {Ecological Modelling},
    volume =     {202},
    number =     {3},
    pages =      {385-396},
    year =       {2007},
    issn =       {0304-3800},
    doi =        {https://doi.org/10.1016/j.ecolmodel.2006.11.005},
    url =        {https://www.sciencedirect.com/science/article/pii/S0304380006005400},
    author =     {Craig A. Aumann},
    keywords =   {Simulation, Individual-based model, Hierarchy theory, Assessment, Scientific explanation, Emergent property, Complexity},
    abstract =   {While the complexity of ecological simulation models has increased along with advances in computer hardware, recent reviews have concluded that such complex models have generally not fulfilled their potential for advancing ecological understanding. This state of affairs is partially the result of little attention being given to methodological issues for developing simulation models. This paper presents one methodology for developing models of complex systems. The first part of this methodology places the modeling process in the context of general research planning and thus emphasizes the process of synthesis—combining the constituent pieces of knowledge into a unified description of the entities and processes comprising the system to be modeled. The second methodological step involves operationalizing this synthetic description into a composition of smaller models specified over three scalar hierarchical levels. The hierarchical structure of the decomposition also structures the explanations given about system behaviors in that the “mechanism” for a behavior arises at the lower levels while its “purpose” is found at higher levels. The third part of the methodology involves using model assessment to explicitly establish the veracity of the links between the implemented model, the model design specification, and the synthesis. The veracity of these links are established by demonstrating: (i) that the model has been built correctly relative to the model design (verification), (ii) that the right model has been built as judged by the behaviors of the model components across the hierarchical levels (validation), and (iii) that the assessment criteria used are credible for judging model adequacy for the stated modeling objectives (critique). By creating confidence in the mapping between the model implementation and synthesis, the assessment process enables confidence in the explanations given for model behaviors and the theoretical understanding expressed in the synthesis. Alternatively, the failure of the model to satisfy assessment criteria may necessitate a reformulation of the understanding on which the model is based. Regardless of which possibility occurs, because the reasons for model behaviors can be mapped back to the synthesis, ecological understanding is advanced.}
}
